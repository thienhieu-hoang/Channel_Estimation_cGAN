{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tempfile import TemporaryFile\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Add the Torch_code directory to the Python path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import config\n",
    "import utils\n",
    "import loader\n",
    "# import skfuzzy as fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AT30890\\Hoctap\\Hprediction\\H_est_cGAN\\Channel_Estimation_cGAN_new\\Channel_Estimation_cGAN\\Torch_code\\train\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "print(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AT30890\\Hoctap\\Hprediction\\H_est_cGAN\\Channel_Estimation_cGAN_new\\Channel_Estimation_cGAN\\Torch_code\n"
     ]
    }
   ],
   "source": [
    "print(config.FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "BATCH_SIZE = 32 #64  # Batch size\n",
    "NUM_EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SNR: 0/0\n",
      " Training for LS+LI\n",
      "SNR: 0/0, LS+LI, Epoch 1/1, Loss: 0.11736565436391781 \n",
      "SNR: 0/0, LS+LI, Val Loss: 0.02397922131543358\n",
      "NMSE: 0.08246636390686035\n",
      "NMSE: 0.9816241264343262\n",
      " Training for LS\n",
      "SNR: 0/0, LS, Epoch 1/1, Loss: 0.1151551212145326 \n",
      "SNR: 0/0, LS, Val Loss: 0.033022234526773296\n",
      "NMSE: 1.3504704236984253\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not convert <built-in method cpu of Tensor object at 0x000001785716FF20> (type <class 'builtin_function_or_method'>) to array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 405\u001b[0m\n\u001b[0;32m    402\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mFILE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m'\u001b[39m, rowss, \u001b[38;5;28mstr\u001b[39m(snr) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdB\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;28mstr\u001b[39m(index_save) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_LS_CNN_estimatedChan.png\u001b[39m\u001b[38;5;124m'\u001b[39m) )\n\u001b[0;32m    403\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[1;32m--> 405\u001b[0m \u001b[43msavemat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AT30890\\AppData\\Local\\anaconda3\\envs\\Torch_GPU\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:301\u001b[0m, in \u001b[0;36msavemat\u001b[1;34m(file_name, mdict, appendmat, format, long_field_names, do_compression, oned_as)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormat should be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 301\u001b[0m \u001b[43mMW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AT30890\\AppData\\Local\\anaconda3\\envs\\Torch_GPU\\Lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:892\u001b[0m, in \u001b[0;36mMatFile5Writer.put_variables\u001b[1;34m(self, mdict, write_header)\u001b[0m\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mwrite(out_str)\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# not compressing\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matrix_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_top\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_global\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AT30890\\AppData\\Local\\anaconda3\\envs\\Torch_GPU\\Lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:634\u001b[0m, in \u001b[0;36mVarWriter5.write_top\u001b[1;34m(self, arr, name, is_global)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m    633\u001b[0m \u001b[38;5;66;03m# write the header and data\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AT30890\\AppData\\Local\\anaconda3\\envs\\Torch_GPU\\Lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:654\u001b[0m, in \u001b[0;36mVarWriter5.write\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    652\u001b[0m narr \u001b[38;5;241m=\u001b[39m to_writeable(arr)\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m narr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) to array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(narr, MatlabObject):\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_object(narr)\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert <built-in method cpu of Tensor object at 0x000001785716FF20> (type <class 'builtin_function_or_method'>) to array"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# rows from DeepMIMO dataset settings\n",
    "# change rows according to the .mat dataset file \n",
    "rows = [['550', '568']] # , ['5100','5109']\n",
    "rowss = \"550_568\"\n",
    "SNR = np.arange(0, 36, 5) # 0:5:35 dB\n",
    "outer_file_path = os.path.join(config.FILE_PATH, '..', 'DeepMIMOv2', 'Gan_Data', 'Static_612x14', 'freq_symb_1ant_612sub')\n",
    "for snr in SNR:\n",
    "\n",
    "    print(f\" SNR: {snr}/{SNR[-1]}\")\n",
    "\n",
    "    H_true = np.empty((0, 2, 612, 14)) # true channel\n",
    "\n",
    "    H_equal = np.empty((0, 2, 612, 14)) # noisy channel # LS channel\n",
    "    H_linear = np.empty((0, 2, 612, 14)) # noisy channel # LS+Linear Interpolated channel\n",
    "    H_practical = np.empty((0, 2, 612, 14)) # noisy channel # Practical Estimated channel\n",
    "\n",
    "    # read data from ifferent .mat file, then concatenate them\n",
    "    for i in range(len(rows)):\n",
    "        file_path_partial = 'Gan_' + str(snr) +'_dBOutdoor1_60_1ant_612subcs_Row_' + rows[i][0] +'_' + rows[i][1] + '.mat'\n",
    "\n",
    "        file_path = os.path.join(outer_file_path, file_path_partial)\n",
    "        file_path = os.path.normpath(file_path)\n",
    "        file = h5py.File(file_path, 'r')\n",
    "        \n",
    "        H_true = np.concatenate((H_true, np.array(file['H_data'])), axis = 0) # N_samples x channel(2) x height(614) x width(14)\n",
    "        H_equal = np.concatenate((H_equal, np.array(file['H_equalized_data'])), axis = 0)\n",
    "        H_linear = np.concatenate((H_linear, np.array(file['H_linear_data'])), axis=0)\n",
    "        H_practical = np.concatenate((H_practical, np.array(file['H_practical_data'])), axis=0)\n",
    "\n",
    "    shuffle_order = np.random.permutation(H_true.shape[0]);\n",
    "    H_true = torch.tensor(H_true[shuffle_order])\n",
    "    H_equal = torch.tensor(H_equal[shuffle_order])\n",
    "    H_linear = torch.tensor(H_linear[shuffle_order])\n",
    "    H_practical = torch.tensor(H_practical[shuffle_order])\n",
    "\n",
    "    train_size = np.floor(H_practical.shape[0]*0.9) //BATCH_SIZE *BATCH_SIZE\n",
    "    # print(train_size)\n",
    "    # print(train_size/64)\n",
    "    # print(train_size/input_data.size(0))\n",
    "    train_size = int(train_size)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1. When input is H_linear (after LS+LI)\n",
    "    print(f\" Training for LS+LI\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    # 1.1 Split into training and validation sets for H_NN training\n",
    "    trainData   = H_linear[0:train_size,:,:,:].to(device, dtype=torch.float)\n",
    "    trainLabels = H_true[0:train_size,:,:,:].to(device, dtype=torch.float)\n",
    "\n",
    "    valData   = H_linear[train_size:,:,:,:].to(device, dtype=torch.float)\n",
    "    valLabels = H_true[train_size:,:,:,:].to(device, dtype=torch.float)\n",
    "\n",
    "    # Split H_equal, H_linear, H_practical for validation later\n",
    "    H_equal_val = H_equal[train_size:,:,:,:].to(device, dtype=torch.float)\n",
    "    H_linear_val = H_linear[train_size:,:,:,:].to(device, dtype=torch.float)\n",
    "    H_practical_val = H_practical[train_size:,:,:,:].to(device, dtype=torch.float)\n",
    "\n",
    "    # 1.2 Normalization\n",
    "    trainData_min = trainData.min()\n",
    "    trainData_max = trainData.max()\n",
    "    trainLabels_min = trainLabels.min()\n",
    "    trainLabels_max = trainLabels.max()\n",
    "\n",
    "    trainData_normd   = (trainData - trainData_min)/ (trainData_max - trainData_min)\n",
    "    trainLabels_normd = (trainLabels - trainLabels_min)/ (trainLabels_max - trainLabels_min)\n",
    "    valData_normd     = (valData - trainData_min)/ (trainData_max - trainData_min)\n",
    "    valLabels_normd   = (valLabels - trainLabels_min)/ (trainLabels_max - trainLabels_min)\n",
    "    # for evaluation, output of model(valData) will be de-normalized and compared with valLabels\n",
    "\n",
    "    # Split real and imaginary grids into 2 image sets, then concatenate\n",
    "    trainData_normd   = torch.cat((trainData_normd[:,0,:,:], trainData_normd[:,1,:,:]), dim=0).unsqueeze(1)  # 612 x 14 x (Nsamples*2)\n",
    "    trainLabels_normd = torch.cat((trainLabels_normd[:,0,:,:], trainLabels_normd[:,1,:,:]), dim=0).unsqueeze(1)  # 612 x 14 x (Nsamples*2)\n",
    "\n",
    "    # 1.3 Create a DataLoader for dataset\n",
    "    dataset = TensorDataset(trainData_normd, trainLabels_normd)  # [4224, 1, 612, 14]\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(valData_normd, valLabels_normd)  # [241, 2, 612, 14]\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 1.4 model\n",
    "    model = utils.CNN_Est().to(device)\n",
    "\n",
    "    learning_rate = 0.00001\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 1.5 Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1):\n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    i = i+val_outputs_imag.size(0)\n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS+LI, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "    save_folder = os.path.join(config.FILE_PATH, 'model/static/CNN', rowss, str(snr)+'dB')\n",
    "    index_save = loader.find_incremental_filename(save_folder, 'CNN_', '_variable')\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_LI_CNN_model.pth')\n",
    "    variable_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_variable.pth')\n",
    "    params_save_path = os.path.join(save_folder, 'CNN_' +str(index_save)+'_params.mat')\n",
    "    \n",
    "    params = {   \n",
    "                'SNR': snr,\n",
    "                'epoc': NUM_EPOCHS,\n",
    "                'rows': rowss,\n",
    "                'learning_rate': learning_rate\n",
    "    }\n",
    "    savemat(params_save_path, params)\n",
    "    variables = {             \n",
    "                'train_track_LI': train_loss,\n",
    "                'val_track_LI': val_loss,\n",
    "                'train_min_LI': trainData_min.cpu(),\n",
    "                'train_max_LI': trainData_max.cpu(),\n",
    "                'val_min': trainLabels_min.cpu(),\n",
    "                'val_max': trainLabels_max.cpu,\n",
    "    }\n",
    "    # variable to save \n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, model_save_path)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, 'results', 'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_LS_LI_Loss.png') )\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    # True channel\n",
    "    H_val_true = valLabels.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_true_complex = torch.complex(H_val_true[:,0,:,:], H_val_true[:,1,:,:])\n",
    "    # variables['H_val_true'] = H_val_true # (nVal, 2, 612, 14)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(H_val_true[-1,0,:,:],  aspect='auto', cmap='viridis', interpolation='none')\n",
    "    plt.xlabel('OFDM symbol')\n",
    "    plt.ylabel('Subcarrier')\n",
    "    plt.title('True Channel')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, 'results',  'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_trueChannel.png') )\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    # Linear interpolated channel\n",
    "    H_val_linInterp = valData.cpu()\n",
    "    # convert to complex matrices\n",
    "    H_val_linInterp_complex = torch.complex(H_val_linInterp[:,0,:,:], H_val_linInterp[:,1,:,:])\n",
    "\n",
    "    # NMSE of Linear Interpolation\n",
    "    # Calculate the mean squared error\n",
    "    mse_LI = torch.mean(torch.abs(H_val_true_complex - H_val_linInterp_complex) ** 2)\n",
    "    # Calculate the variance of the reference tensor (complex_tensor1)\n",
    "    variance = torch.var(H_val_true_complex)\n",
    "    # Calculate the NMSE\n",
    "    nmse_LI = mse_LI / variance\n",
    "    variables['NMSE_LI'] = nmse_LI.cpu()\n",
    "    print(f\"NMSE: {nmse_LI.item()}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(H_val_linInterp[-1,0,:,:],  aspect='auto', cmap='viridis', interpolation='none')\n",
    "    plt.xlabel('OFDM symbol')\n",
    "    plt.ylabel('Subcarrier')\n",
    "    plt.title(f'LS + Interpolate Estimated Channel, NMSE: {nmse_LI:.4f}')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, 'results', 'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_LS_LI_estimatedChan.png') )\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # Estimated Channel \n",
    "    H_val_NN = H_NN_val.cpu()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(H_val_NN[-1,0,:,:],  aspect='auto', cmap='viridis', interpolation='none')\n",
    "    plt.xlabel('OFDM symbol')\n",
    "    plt.ylabel('Subcarrier')\n",
    "    plt.title('LI+CNN Estimated Channel (before de-normlized)')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, \"results\", 'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_LS_LI_CNN_estimatedChan_before_denorm.png') )\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    # De-normalized\n",
    "    H_val_NN_denormd = H_NN_val * (trainLabels_max - trainLabels_min) + trainLabels_min\n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "\n",
    "    # variables['H_val_LI_NN'] = H_val_NN_denormd # (nVal, 2, 612, 14)\n",
    "\n",
    "    # convert to complex matrices\n",
    "    H_val_NN_denormd_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "\n",
    "    # NMSE of Linear Interpolation + NN\n",
    "    # Calculate the mean squared error\n",
    "    mse_LI_NN = torch.mean(torch.abs(H_val_true_complex - H_val_NN_denormd_complex) ** 2)\n",
    "    # Calculate the NMSE\n",
    "    nmse_LI_NN = mse_LI_NN / variance\n",
    "    print(f\"NMSE: {nmse_LI_NN.item()}\")\n",
    "    variables['NMSE_LI_NN'] = nmse_LI_NN.cpu()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(H_val_NN_denormd[-1,0,:,:],  aspect='auto', cmap='viridis', interpolation='none')\n",
    "    plt.xlabel('OFDM symbol')\n",
    "    plt.ylabel('Subcarrier')\n",
    "    plt.title(f'LI+CNN Estimated Channel (after de-normlized), NMSE: {nmse_LI_NN:.4f}')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, 'results', 'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_LS_LI_CNN_estimatedChan.png') )\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # When Input of the NN is just H_equalized\n",
    "    print(f\" Training for LS\")\n",
    "    # [samples, 2, 612, 14]\n",
    "    # Split into training and validation sets for H_NN training\n",
    "    trainData   = H_equal[0:train_size,:,:,:].to(device, dtype=torch.float)\n",
    "    trainLabels = H_true[0:train_size,:,:,:].to(device, dtype=torch.float)\n",
    "\n",
    "    valData   = H_equal[train_size:,:,:,:].to(device, dtype=torch.float)\n",
    "    valLabels = H_true[train_size:,:,:,:].to(device, dtype=torch.float)\n",
    "\n",
    "    # Normalization\n",
    "    trainData_min = trainData.min()\n",
    "    trainData_max = trainData.max()\n",
    "    trainLabels_min = trainLabels.min()\n",
    "    trainLabels_max = trainLabels.max()\n",
    "\n",
    "    trainData_normd   = (trainData - trainData_min)/ (trainData_max - trainData_min)\n",
    "    trainLabels_normd = (trainLabels - trainLabels_min)/ (trainLabels_max - trainLabels_min)\n",
    "    valData_normd     = (valData - trainData_min)/ (trainData_max - trainData_min)\n",
    "    valLabels_normd   = (valLabels - trainLabels_min)/ (trainLabels_max - trainLabels_min)\n",
    "\n",
    "\n",
    "    # Split real and imaginary grids into 2 image sets, then concatenate\n",
    "    trainData_normd   = torch.cat((trainData_normd[:,0,:,:], trainData_normd[:,1,:,:]), dim=0).unsqueeze(1)  # 612 x 14 x (Nsamples*2)\n",
    "    trainLabels_normd = torch.cat((trainLabels_normd[:,0,:,:], trainLabels_normd[:,1,:,:]), dim=0).unsqueeze(1)  # 612 x 14 x (Nsamples*2)\n",
    "\n",
    "    H_temp = trainData.cpu()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(H_temp[0,0,:,:],  aspect='auto', cmap='viridis', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('OFDM symbol')\n",
    "    plt.ylabel('Subcarrier')\n",
    "    plt.title(f'LS Channel')\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, 'results', 'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_LS_Chan.png') )\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    # Create a DataLoader for dataset\n",
    "    dataset = TensorDataset(trainData_normd, trainLabels_normd)  # [4224, 1, 612, 14]\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(valData_normd, valLabels_normd)  # [241, 2, 612, 14]\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model2 = utils.CNN_Est().to(device)\n",
    "    learning_rate = 0.00001\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    train_loss =[]\n",
    "    val_loss = []\n",
    "    H_NN_val = torch.empty_like(valLabels) # [nVal, 2, 612, 14]\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        model2.train()\n",
    "        running_loss = 0.0\n",
    "        if (epoch == num_epochs-1):\n",
    "            i = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer2.zero_grad()\n",
    "            outputs = model2(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss} \")\n",
    "        \n",
    "        # Validation \n",
    "        model2.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs_real = val_inputs[:,0,:,:].unsqueeze(1)\n",
    "                val_inputs_imag = val_inputs[:,1,:,:].unsqueeze(1)\n",
    "                val_targets_real = val_targets[:,0,:,:].unsqueeze(1)\n",
    "                val_targets_imag = val_targets[:,1,:,:].unsqueeze(1)\n",
    "                \n",
    "                val_outputs_real = model2(val_inputs_real)\n",
    "                val_loss_real = criterion(val_outputs_real, val_targets_real)\n",
    "                running_val_loss += val_loss_real.item()\n",
    "                \n",
    "                val_outputs_imag = model2(val_inputs_imag)\n",
    "                val_loss_imag = criterion(val_outputs_imag, val_targets_imag)\n",
    "                running_val_loss += val_loss_imag.item()\n",
    "                \n",
    "                if (epoch == num_epochs-1):\n",
    "                    H_NN_val[i:i+val_outputs_real.size(0),0,:,:].unsqueeze(1).copy_(val_outputs_real)\n",
    "                    H_NN_val[i:i+val_outputs_imag.size(0),1,:,:].unsqueeze(1).copy_(val_outputs_imag)\n",
    "                    i = i+val_outputs_imag.size(0)\n",
    "                \n",
    "        avg_val_loss = running_val_loss / (len(val_loader)*2)\n",
    "        val_loss.append(avg_val_loss)    \n",
    "                \n",
    "        print(f\"SNR: {snr}/{SNR[-1]}, LS, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, 'results', 'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_LS_Loss.png') )\n",
    "    plt.clf()\n",
    "\n",
    "    H_val_NN_denormd = H_NN_val * (trainLabels_max - trainLabels_min) + trainLabels_min\n",
    "    H_val_NN_denormd = H_val_NN_denormd.cpu()\n",
    "\n",
    "    model_save_path = os.path.join(save_folder,  'CNN_' +str(index_save)+'_LS_CNN_model.pth')\n",
    "\n",
    "    # variables['H_val_LS_NN']= H_val_NN_denormd.cpu() # (nVal, 2, 612, 14)\n",
    "    variables['train_track_LS']= train_loss\n",
    "    variables['val_track_LS']= val_loss\n",
    "    variables['train_min_LS']= trainData_min.cpu()\n",
    "    variables['train_max_LS']= trainData_max.cpu()\n",
    "\n",
    "    # variable to save \n",
    "    # Save the models' state dictionaries \n",
    "    torch.save({'model_state_dict': model2.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, model_save_path)\n",
    "\n",
    "\n",
    "    # NMSE of LS + NN\n",
    "    H_val_LS_NN_complex = torch.complex(H_val_NN_denormd[:,0,:,:], H_val_NN_denormd[:,1,:,:])\n",
    "    # Calculate the mean squared error\n",
    "    mse_LS_NN = torch.mean(torch.abs(H_val_true_complex - H_val_LS_NN_complex) ** 2)\n",
    "    # Calculate the NMSE\n",
    "    nmse_LS_NN = mse_LS_NN / variance\n",
    "    print(f\"LS+CNN NMSE: {nmse_LS_NN.item()}\")\n",
    "    variables['NMSE_LS_NN'] = nmse_LS_NN.cpu()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(H_val_NN_denormd[-1,0,:,:],  aspect='auto', cmap='viridis', interpolation='none')\n",
    "    plt.xlabel('OFDM symbol')\n",
    "    plt.ylabel('Subcarrier')\n",
    "    plt.title(f'LS+CNN Estimated Channel (after de-normlized), NMSE: {nmse_LS_NN:.4f}')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(config.FILE_PATH, 'results', 'static', 'CNN', rowss, str(snr) + 'dB',  str(index_save) + '_LS_CNN_estimatedChan.png') )\n",
    "    plt.clf()\n",
    "\n",
    "    torch.save( variables,variable_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SNR': 0, 'epoc': 20, 'rows': '550_568', 'learning_rate': 1e-05, 'train_track_LI': [0.12542229983955622, 0.016818697966906864, 0.006570089958889487, 0.0017958001614412449, 0.0010687704604303387, 0.0006360203134742429, 0.0004226578427430165, 0.000339401617641973, 0.00029624920997169585, 0.00026810644628009567, 0.0002501570282523365, 0.00023476741269708631, 0.0002248910150986679, 0.00021715459956794803, 0.00021035958222152354, 0.00020660342367288345, 0.00019989069134377738, 0.00019464028150650847, 0.00019122100115964713, 0.00018718416041944388], 'val_track_LI': [0.028704328772922356, 0.01160938641987741, 0.0027627505478449166, 0.0013673651604525123, 0.0008469064535650735, 0.0005033311208535451, 0.0003792883859811506, 0.00032053827332371537, 0.00029038065440545324, 0.00026359001640230417, 0.00025747243004540604, 0.00023620072109527732, 0.0002252046312302506, 0.00022641037867288105, 0.0002112559110779936, 0.0002063743898664446, 0.00020225443404342514, 0.00019736855877757384, 0.00019304889610793907, 0.0001906198000748797], 'train_min_LI': tensor(-2.2338e-06, device='cuda:0'), 'train_max_LI': tensor(1.5294e-06, device='cuda:0'), 'val_min': tensor(-5.3256e-07, device='cuda:0'), 'val_max': tensor(5.3266e-07, device='cuda:0'), 'NMSE_LI': tensor(0.0828), 'NMSE_LI_NN': tensor(0.0071), 'H_val_LS_NN': tensor([[[[-1.5693e-07, -1.3738e-07, -8.0280e-08,  ..., -3.9861e-08,\n",
      "           -9.2171e-08, -1.4545e-07],\n",
      "          [-1.2300e-07, -1.2390e-07, -1.0826e-07,  ..., -5.9200e-08,\n",
      "           -6.9611e-08, -9.5470e-08],\n",
      "          [-1.1514e-07, -1.3042e-07, -1.2567e-07,  ..., -8.2031e-08,\n",
      "           -6.3485e-08, -5.5495e-08],\n",
      "          ...,\n",
      "          [ 8.3253e-08,  1.1420e-07,  8.2593e-08,  ...,  1.1321e-07,\n",
      "            1.1645e-07,  8.4898e-08],\n",
      "          [ 1.1728e-08,  4.9471e-08,  5.7555e-08,  ...,  9.5033e-08,\n",
      "            8.4412e-08,  3.6459e-08],\n",
      "          [-9.1061e-08, -9.2238e-09,  4.1867e-08,  ...,  7.0843e-08,\n",
      "            4.5306e-08, -3.1016e-08]],\n",
      "\n",
      "         [[-1.6304e-08,  3.4429e-08,  1.1166e-07,  ...,  1.0229e-07,\n",
      "            1.3879e-08, -5.9032e-08],\n",
      "          [ 8.5516e-08,  1.2374e-07,  1.5741e-07,  ...,  1.2782e-07,\n",
      "            8.9644e-08,  3.8778e-08],\n",
      "          [ 1.4022e-07,  1.6300e-07,  1.8613e-07,  ...,  1.5252e-07,\n",
      "            1.4490e-07,  1.3606e-07],\n",
      "          ...,\n",
      "          [-5.4824e-08, -4.7665e-08, -7.3904e-08,  ..., -9.7947e-08,\n",
      "           -7.7475e-08, -9.3211e-08],\n",
      "          [-8.2778e-08, -7.4122e-08, -7.0561e-08,  ..., -7.8208e-08,\n",
      "           -6.5381e-08, -1.0555e-07],\n",
      "          [-1.5258e-07, -9.6463e-08, -5.8639e-08,  ..., -5.6202e-08,\n",
      "           -6.6519e-08, -1.3158e-07]]],\n",
      "\n",
      "\n",
      "        [[[-1.9478e-07, -1.8412e-07, -1.2991e-07,  ..., -9.5167e-08,\n",
      "           -1.3376e-07, -1.8365e-07],\n",
      "          [-1.8117e-07, -1.8891e-07, -1.7850e-07,  ..., -1.3115e-07,\n",
      "           -1.3518e-07, -1.5582e-07],\n",
      "          [-1.8795e-07, -2.0265e-07, -2.1047e-07,  ..., -1.7093e-07,\n",
      "           -1.5191e-07, -1.3669e-07],\n",
      "          ...,\n",
      "          [-5.9537e-08, -5.2941e-08, -7.8629e-08,  ..., -9.1498e-08,\n",
      "           -7.0898e-08, -8.6910e-08],\n",
      "          [-8.6067e-08, -7.8039e-08, -7.4290e-08,  ..., -7.2117e-08,\n",
      "           -5.9450e-08, -9.9446e-08],\n",
      "          [-1.5471e-07, -9.9267e-08, -6.1623e-08,  ..., -5.1463e-08,\n",
      "           -6.1666e-08, -1.2703e-07]],\n",
      "\n",
      "         [[-5.9082e-08, -1.7504e-08,  5.3045e-08,  ...,  4.0929e-08,\n",
      "           -3.3311e-08, -9.6011e-08],\n",
      "          [ 2.3806e-08,  4.8657e-08,  7.7504e-08,  ...,  4.6580e-08,\n",
      "            2.0506e-08, -1.7100e-08],\n",
      "          [ 6.2427e-08,  7.0015e-08,  8.7301e-08,  ...,  5.4772e-08,\n",
      "            6.0384e-08,  5.7454e-08],\n",
      "          ...,\n",
      "          [ 4.2315e-09,  1.8283e-08, -1.1946e-08,  ..., -6.7369e-08,\n",
      "           -4.9790e-08, -6.9280e-08],\n",
      "          [-4.0512e-08, -2.0469e-08, -1.8802e-08,  ..., -5.0415e-08,\n",
      "           -4.2399e-08, -8.3577e-08],\n",
      "          [-1.2432e-07, -5.7386e-08, -1.4667e-08,  ..., -3.5100e-08,\n",
      "           -4.8327e-08, -1.1532e-07]]],\n",
      "\n",
      "\n",
      "        [[[-7.2806e-08, -3.4475e-08,  3.3609e-08,  ...,  3.7991e-08,\n",
      "           -3.4684e-08, -9.7239e-08],\n",
      "          [ 1.2786e-09,  2.0133e-08,  4.6928e-08,  ...,  3.8979e-08,\n",
      "            1.5246e-08, -2.1995e-08],\n",
      "          [ 2.9734e-08,  3.2273e-08,  4.6560e-08,  ...,  4.1062e-08,\n",
      "            4.9495e-08,  4.5977e-08],\n",
      "          ...,\n",
      "          [ 1.0604e-08,  2.6645e-08, -3.2151e-09,  ..., -2.8243e-08,\n",
      "           -1.0729e-08, -3.3234e-08],\n",
      "          [-3.7270e-08, -1.5766e-08, -1.3229e-08,  ..., -1.9858e-08,\n",
      "           -1.4418e-08, -5.6198e-08],\n",
      "          [-1.2285e-07, -5.5033e-08, -1.1617e-08,  ..., -1.3746e-08,\n",
      "           -2.8325e-08, -9.6160e-08]],\n",
      "\n",
      "         [[-8.3313e-08, -4.7457e-08,  1.8727e-08,  ...,  2.4528e-08,\n",
      "           -4.4843e-08, -1.0559e-07],\n",
      "          [-1.4928e-08,  2.7313e-10,  2.5522e-08,  ...,  2.0781e-08,\n",
      "           -3.4419e-10, -3.5719e-08],\n",
      "          [ 8.6478e-09,  8.1704e-09,  2.0887e-08,  ...,  1.6991e-08,\n",
      "            2.8582e-08,  2.5569e-08],\n",
      "          ...,\n",
      "          [ 3.3007e-08,  5.3034e-08,  2.2094e-08,  ...,  2.9432e-09,\n",
      "            1.7518e-08, -6.4024e-09],\n",
      "          [-2.1845e-08,  4.5071e-09,  8.1293e-09,  ...,  5.1649e-09,\n",
      "            7.8743e-09, -3.5467e-08],\n",
      "          [-1.1265e-07, -4.0537e-08,  5.0628e-09,  ...,  4.3772e-09,\n",
      "           -1.1874e-08, -8.0984e-08]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.2636e-07, -2.2234e-07, -1.7229e-07,  ..., -1.5391e-07,\n",
      "           -1.8357e-07, -2.2960e-07],\n",
      "          [-2.2564e-07, -2.3815e-07, -2.3073e-07,  ..., -2.1061e-07,\n",
      "           -2.0450e-07, -2.2426e-07],\n",
      "          [-2.4172e-07, -2.5755e-07, -2.7414e-07,  ..., -2.5883e-07,\n",
      "           -2.4192e-07, -2.2346e-07],\n",
      "          ...,\n",
      "          [ 2.1639e-07,  2.7851e-07,  2.4720e-07,  ...,  2.2509e-07,\n",
      "            2.1096e-07,  1.6842e-07],\n",
      "          [ 1.0438e-07,  1.6863e-07,  1.9273e-07,  ...,  1.8821e-07,\n",
      "            1.5981e-07,  1.0055e-07],\n",
      "          [-2.9845e-08,  7.8405e-08,  1.4544e-07,  ...,  1.3968e-07,\n",
      "            1.0207e-07,  1.3035e-08]],\n",
      "\n",
      "         [[-1.1182e-07, -8.2206e-08, -1.9601e-08,  ..., -7.1150e-09,\n",
      "           -6.8994e-08, -1.2566e-07],\n",
      "          [-5.2185e-08, -4.2340e-08, -2.0644e-08,  ..., -1.5736e-08,\n",
      "           -3.2184e-08, -6.3101e-08],\n",
      "          [-2.8003e-08, -3.5132e-08, -2.4111e-08,  ..., -2.3892e-08,\n",
      "           -8.5320e-09, -7.7695e-09],\n",
      "          ...,\n",
      "          [-5.1649e-08, -4.5777e-08, -7.3918e-08,  ..., -1.8165e-07,\n",
      "           -1.6209e-07, -1.7297e-07],\n",
      "          [-8.1849e-08, -7.3634e-08, -7.1957e-08,  ..., -1.4555e-07,\n",
      "           -1.3281e-07, -1.6802e-07],\n",
      "          [-1.5258e-07, -9.7072e-08, -6.0490e-08,  ..., -1.0695e-07,\n",
      "           -1.1585e-07, -1.7496e-07]]],\n",
      "\n",
      "\n",
      "        [[[-1.0195e-07, -7.0923e-08, -7.8102e-09,  ...,  5.1683e-09,\n",
      "           -5.9263e-08, -1.1757e-07],\n",
      "          [-4.2974e-08, -3.2265e-08, -1.0140e-08,  ..., -2.9615e-09,\n",
      "           -2.0643e-08, -5.3130e-08],\n",
      "          [-2.3935e-08, -2.9366e-08, -1.8609e-08,  ..., -1.1647e-08,\n",
      "            3.2424e-09,  2.7492e-09],\n",
      "          ...,\n",
      "          [ 9.5277e-09,  2.5563e-08, -4.1606e-09,  ..., -3.1479e-08,\n",
      "           -1.3749e-08, -3.6224e-08],\n",
      "          [-3.8733e-08, -1.7470e-08, -1.4996e-08,  ..., -2.3130e-08,\n",
      "           -1.7473e-08, -5.9075e-08],\n",
      "          [-1.2418e-07, -5.6862e-08, -1.3706e-08,  ..., -1.6472e-08,\n",
      "           -3.0876e-08, -9.8540e-08]],\n",
      "\n",
      "         [[-7.7363e-08, -3.9906e-08,  2.7603e-08,  ...,  3.3629e-08,\n",
      "           -3.8066e-08, -1.0002e-07],\n",
      "          [-5.0240e-09,  1.2481e-08,  3.8852e-08,  ...,  3.3040e-08,\n",
      "            1.0030e-08, -2.6625e-08],\n",
      "          [ 2.2302e-08,  2.3618e-08,  3.7425e-08,  ...,  3.3080e-08,\n",
      "            4.2496e-08,  3.9067e-08],\n",
      "          ...,\n",
      "          [-1.7870e-08, -6.4493e-09, -3.5115e-08,  ..., -7.0024e-08,\n",
      "           -5.1219e-08, -6.9990e-08],\n",
      "          [-5.6731e-08, -4.1112e-08, -3.9103e-08,  ..., -5.3982e-08,\n",
      "           -4.4791e-08, -8.5458e-08],\n",
      "          [-1.3551e-07, -7.2946e-08, -3.2359e-08,  ..., -3.8391e-08,\n",
      "           -5.0767e-08, -1.1716e-07]]],\n",
      "\n",
      "\n",
      "        [[[-7.5487e-08, -3.6999e-08,  3.1472e-08,  ...,  6.0019e-08,\n",
      "           -1.7727e-08, -8.3391e-08],\n",
      "          [-1.9447e-09,  1.7227e-08,  4.4912e-08,  ...,  6.8019e-08,\n",
      "            4.1084e-08,  2.9223e-10],\n",
      "          [ 2.6802e-08,  2.9605e-08,  4.5023e-08,  ...,  7.8421e-08,\n",
      "            8.2874e-08,  7.8734e-08],\n",
      "          ...,\n",
      "          [ 2.7608e-08,  4.6142e-08,  1.5106e-08,  ..., -1.5786e-08,\n",
      "            3.9279e-10, -2.2829e-08],\n",
      "          [-2.4487e-08,  6.4171e-10,  3.8276e-09,  ..., -8.8301e-09,\n",
      "           -4.6314e-09, -4.7349e-08],\n",
      "          [-1.1378e-07, -4.2357e-08,  2.8182e-09,  ..., -5.1458e-09,\n",
      "           -2.0743e-08, -8.9296e-08]],\n",
      "\n",
      "         [[-5.1523e-08, -8.5163e-09,  6.3071e-08,  ...,  6.0616e-08,\n",
      "           -1.7635e-08, -8.3783e-08],\n",
      "          [ 3.1836e-08,  5.8043e-08,  8.7665e-08,  ...,  6.7752e-08,\n",
      "            4.0038e-08, -1.2530e-09],\n",
      "          [ 6.7333e-08,  7.6764e-08,  9.4655e-08,  ...,  7.6618e-08,\n",
      "            8.0357e-08,  7.5183e-08],\n",
      "          ...,\n",
      "          [ 1.6251e-07,  2.1195e-07,  1.7807e-07,  ...,  1.8880e-07,\n",
      "            1.8029e-07,  1.4223e-07],\n",
      "          [ 6.7722e-08,  1.2229e-07,  1.4013e-07,  ...,  1.5944e-07,\n",
      "            1.3674e-07,  8.1404e-08],\n",
      "          [-5.3691e-08,  4.4692e-08,  1.0600e-07,  ...,  1.1901e-07,\n",
      "            8.5517e-08,  2.6779e-10]]]]), 'train_track_LS': [0.09253137288033031, 0.02707118182055031, 0.022697088119457476, 0.01688235917633089, 0.004919052914071169, 0.001491880999613689, 0.0008795766161711072, 0.0006376907819382419, 0.000525684531112347, 0.0004552998026762604, 0.00040890439307380194, 0.0003842583566135242, 0.0003557589483686267, 0.0003430423846566555, 0.00033121222857820004, 0.0003214094568496269, 0.00031454122707449034, 0.0003094477513059246, 0.00030568611002005736, 0.0002940441999423153], 'val_track_LS': [0.03515757263327638, 0.024704243910188477, 0.021781118974710505, 0.010815930513975522, 0.002192846043423439, 0.0010950490250252187, 0.0007332135161656576, 0.0005701962860863811, 0.0004859864784521051, 0.0004385589236335363, 0.0003927061055340649, 0.000374694349981534, 0.00035232724136828136, 0.00034168312898448977, 0.00034002182898499694, 0.0003184652559866663, 0.00031024163460339577, 0.0003316323151617932, 0.00030048692860873416, 0.000313391944776716], 'train_min_LS': tensor(-8.1074e-07, device='cuda:0'), 'train_max_LS': tensor(7.6044e-07, device='cuda:0'), 'NMSE_LS_NN': tensor(0.0117)}\n"
     ]
    }
   ],
   "source": [
    "print(variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
